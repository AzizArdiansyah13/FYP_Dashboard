# -*- coding: utf-8 -*-
"""FYP_Aziz_StockPrediction_ML_Updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TC1P3Mq4NJFAM06TWDIMnNkU1ax4bjSo

**Loading The Data**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas_datareader as data
import yfinance as yf
# %matplotlib inline

import yfinance as yf

start = "2010-01-01"
end = "2020-12-31"

df = yf.download("AAPL", start=start, end=end)
print(df.head())

"""**Downloading the Datasets from Yahoo Finance (2010-2020) including Apple, SAP, and Samsung**"""

import yfinance as yf

# Date range
start = "2010-01-01"
end = "2020-12-31"

# Apple (already done)
apple_df = yf.download("AAPL", start=start, end=end)
print("Apple stock data:")
print(apple_df.head())

# SAP
sap_df = yf.download("SAP", start=start, end=end)
print("\nSAP stock data:")
print(sap_df.head())

# Samsung (KRX: 005930)
samsung_df = yf.download("005930.KS", start=start, end=end)
print("\nSamsung stock data:")
print(samsung_df.head())

apple_df = apple_df.reset_index()

"""**APPLE stock for first example of Machine Learning Model**"""

df.tail()

df = df.reset_index()
df.head()

df = df.drop(['Date', 'Volume'], axis = 1)
df.head()

plt.plot(df.High_AAPL)

print(df.columns)

df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df.columns]

print(df.columns)

import matplotlib.pyplot as plt

plt.plot(df["Close_AAPL"])
plt.show()

print(df.columns)

plt.plot(df["Close_AAPL"])
plt.show()

df

"""**Were testing the Moving Average for the Apple Stock**"""

ma100 = df.Close_AAPL.rolling(100).mean()
ma100

plt.figure(figsize=(12,6))
plt.plot(df.Close_AAPL)
plt.plot(ma100, 'r')

ma200 = df.Close_AAPL.rolling(200).mean()
ma200

plt.figure(figsize=(12,6))
plt.plot(df.Close_AAPL)
plt.plot(ma100, 'r')
plt.plot(ma200, 'g')

df.shape

"""**Splitting the Data into Training and Testing**"""

data_training = pd.DataFrame(df['Close_AAPL'][0:int(len(df)*0.70)])
data_testing = pd.DataFrame(df['Close_AAPL'][int(len(df) * 0.70):])

print(data_training.shape)
print(data_testing.shape)

data_training.head()

data_testing.head()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))

data_training_array = scaler.fit_transform(data_training)
data_training_array

data_training_array.shape

x_train = []
y_train = []

for i in range(100, data_training_array.shape[0]):
    x_train.append(data_training_array[i-100: i])
    y_train.append(data_training_array[i, 0])

x_train, y_train = np.array(x_train), np.array(y_train)

x_train.shape

"""**Machine Learning Model Test**

"""

from keras.layers import Dense, Dropout, LSTM
from keras.models import Sequential

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense

# Define the model
model = Sequential()

# First LSTM layer
model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))

# Second LSTM layer
model.add(LSTM(units=60, activation='relu', return_sequences=True))
model.add(Dropout(0.3))

# Third LSTM layer
model.add(LSTM(units=80, activation='relu', return_sequences=True))
model.add(Dropout(0.3))

# Fourth LSTM layer (final)
model.add(LSTM(units=120, activation='relu'))
model.add(Dropout(0.4))

# Output layer
model.add(Dense(units=1))

# Model summary
model.summary()

"""**Running LSTM Epochs by 100 times**"""

model.compile(optimizer='adam', loss = 'mean_squared_error')
model.fit(x_train, y_train, epochs = 100)

"""**Save the Machine Learning Model **"""

model.save('my_model.keras')

data_testing.head()

data_training.tail(100)

past_100_days = data_training.tail(100)

final_df = pd.concat([past_100_days, data_testing], ignore_index=True)

final_df.head()

input_data = scaler.fit_transform(final_df)
input_data

input_data.shape

x_test = []
y_test = []

for i in range(100, input_data.shape[0]):
    x_test.append(input_data[i-100: i])
    y_test.append(input_data[i, 0])

x_test, y_test = np.array(x_test), np.array(y_test)
print(x_test.shape)
print(y_test.shape)

"""**Making Predictions using Machine Learning Algorithm**"""

y_predicted = model.predict(x_test)

y_predicted.shape

y_test

y_predicted

scaler.scale_

scale_factor = 1/0.00995319
y_predicted = y_predicted * scale_factor
y_test = y_test * scale_factor

import matplotlib.pyplot as plt

plt.figure(figsize=(12,6))
plt.plot(y_test, 'b', label='Original Price')
plt.plot(y_predicted, 'g', label='Predicted Price')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

df.shape

pip install graphviz

"""**Additional Machine Learning Method for FYP (SVR, KNN, and Random Forest)**"""

#SVR METHOD FOR FYP


from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

#KNN METHOD FOR FYP

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

#RANDOM FOREST FOR FYP
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

"""**APPLE Stock Mean Squared Error (MSE) prediction output**"""

# Download AAPL stock data from 2010 to 2020
import yfinance as yf
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

data = yf.download("AAPL", start="2010-01-01", end="2020-12-31")
# Use only the 'Close' price
data = data[['Close']]

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a simple feature: 5-day moving average of the Close price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Remove rows with NaN values (due to shifting and rolling calculations)
data = data.dropna()

# Define features and target
X = data[['Close', 'MA5']]
y = data['Target']

# Split the data into training and testing sets (using no shuffling to respect time series order)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----- Model 1: SVR (Support Vector Regression) -----
svr_model = SVR(kernel='rbf')
svr_model.fit(X_train_scaled, y_train)
svr_pred = svr_model.predict(X_test_scaled)
svr_mse = mean_squared_error(y_test, svr_pred)
print("SVR Mean Squared Error:", svr_mse)

# ----- Model 2: K-Nearest Neighbors Regressor -----
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)
knn_pred = knn_model.predict(X_test_scaled)
knn_mse = mean_squared_error(y_test, knn_pred)
print("KNN Mean Squared Error:", knn_mse)

# ----- Model 3: Random Forest Regressor -----
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
rf_pred = rf_model.predict(X_test_scaled)
rf_mse = mean_squared_error(y_test, rf_pred)
print("Random Forest Mean Squared Error:", rf_mse)

pip install xgboost --default-timeout=100

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR         # Support Vector Regression (for both RBF and linear)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------

# Download historical data for Apple from 2010 to 2020
data = yf.download('AAPL', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a simple feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets while preserving time order
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------

# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVM using SVR with a linear kernel (another SVM variant)
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions and Evaluate
# ---------------------------

# Predict on the test set using each model
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# Calculate Mean Squared Error (MSE) for each model
mse_svr_rbf = mean_squared_error(y_test, pred_svr_rbf)
mse_svr_linear = mean_squared_error(y_test, pred_svr_linear)
mse_knn = mean_squared_error(y_test, pred_knn)
mse_rf = mean_squared_error(y_test, pred_rf)
mse_xgb = mean_squared_error(y_test, pred_xgb)

print("SVR (RBF) MSE:", mse_svr_rbf)
print("SVM (Linear SVR) MSE:", mse_svr_linear)
print("KNN MSE:", mse_knn)
print("Random Forest MSE:", mse_rf)
print("XGBoost MSE:", mse_xgb)

"""**SAP Stock MSE Prediction output**"""

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR         # Support Vector Regression (for both RBF and linear)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor


###APPLE ANALYSIS MODEL (MSE)###

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------

# Download historical data for SAP SE from 2010 to 2020
data = yf.download('SAP.DE', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a simple feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets while preserving time order
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------

# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVM using SVR with a linear kernel (another SVM variant)
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions and Evaluate
# ---------------------------

# Predict on the test set using each model
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# Calculate Mean Squared Error (MSE) for each model
mse_svr_rbf = mean_squared_error(y_test, pred_svr_rbf)
mse_svr_linear = mean_squared_error(y_test, pred_svr_linear)
mse_knn = mean_squared_error(y_test, pred_knn)
mse_rf = mean_squared_error(y_test, pred_rf)
mse_xgb = mean_squared_error(y_test, pred_xgb)

print("SVR (RBF) MSE:", mse_svr_rbf)
print("SVM (Linear SVR) MSE:", mse_svr_linear)
print("KNN MSE:", mse_knn)
print("Random Forest MSE:", mse_rf)
print("XGBoost MSE:", mse_xgb)

"""**SAMSUNG Stock MSE prediction output**"""

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR         # Support Vector Regression (for both RBF and linear)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

###SAMSUNG ANALYSIS MODEL (MSE)###

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------

# Download historical data for Samsung Electronics Co., Ltd. from 2010 to 2020
data = yf.download('005930.KS', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a simple feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets while preserving time order
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------

# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVM using SVR with a linear kernel (another SVM variant)
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions and Evaluate
# ---------------------------

# Predict on the test set using each model
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# Calculate Mean Squared Error (MSE) for each model
mse_svr_rbf = mean_squared_error(y_test, pred_svr_rbf)
mse_svr_linear = mean_squared_error(y_test, pred_svr_linear)
mse_knn = mean_squared_error(y_test, pred_knn)
mse_rf = mean_squared_error(y_test, pred_rf)
mse_xgb = mean_squared_error(y_test, pred_xgb)

print("SVR (RBF) MSE:", mse_svr_rbf)
print("SVM (Linear SVR) MSE:", mse_svr_linear)
print("KNN MSE:", mse_knn)
print("Random Forest MSE:", mse_rf)
print("XGBoost MSE:", mse_xgb)

"""**APPLE Stock MAE Prediction Output**"""

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error  # Use MAE instead of MSE
from sklearn.svm import SVR         # Support Vector Regression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

### APPLE ANALYSIS MODEL (MAE) ###

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------
# Download historical data for Apple from 2010 to 2020
data = yf.download('AAPL', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a simple feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets while preserving time order
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------

# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVR with linear kernel
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions and Evaluate using MAE
# ---------------------------
# Predict on the test set using each model
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# Calculate Mean Absolute Error (MAE) for each model
mae_svr_rbf = mean_absolute_error(y_test, pred_svr_rbf)
mae_svr_linear = mean_absolute_error(y_test, pred_svr_linear)
mae_knn = mean_absolute_error(y_test, pred_knn)
mae_rf = mean_absolute_error(y_test, pred_rf)
mae_xgb = mean_absolute_error(y_test, pred_xgb)

print("SVR (RBF) MAE:", mae_svr_rbf)
print("SVR (Linear) MAE:", mae_svr_linear)
print("KNN MAE:", mae_knn)
print("Random Forest MAE:", mae_rf)
print("XGBoost MAE:", mae_xgb)

"""**Apple Stock MAPE Prediction Output**"""

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_percentage_error  # Import MAPE
from sklearn.svm import SVR         # Support Vector Regression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

### APPLE ANALYSIS MODEL (MAPE) ###

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------
# Download historical data for Apple from 2010 to 2020
data = yf.download('AAPL', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a simple feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets while preserving time order
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------
# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVR with linear kernel
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions and Evaluate using MAPE
# ---------------------------
# Predict on the test set using each model
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# Calculate Mean Absolute Percentage Error (MAPE) for each model
# Multiply by 100 to express as a percentage if desired.
mape_svr_rbf = mean_absolute_percentage_error(y_test, pred_svr_rbf) * 100
mape_svr_linear = mean_absolute_percentage_error(y_test, pred_svr_linear) * 100
mape_knn = mean_absolute_percentage_error(y_test, pred_knn) * 100
mape_rf = mean_absolute_percentage_error(y_test, pred_rf) * 100
mape_xgb = mean_absolute_percentage_error(y_test, pred_xgb) * 100

print("SVR (RBF) MAPE: {:.2f}%".format(mape_svr_rbf))
print("SVR (Linear) MAPE: {:.2f}%".format(mape_svr_linear))
print("KNN MAPE: {:.2f}%".format(mape_knn))
print("Random Forest MAPE: {:.2f}%".format(mape_rf))
print("XGBoost MAPE: {:.2f}%".format(mape_xgb))

"""**Apple Stock Directional Accuracy Output**"""

# Compute actual direction: sign of (actual next day's close - today's close)
actual_direction = np.sign(y_test.values - X_test['Close'].values)

# Compute predicted direction for each model: sign of (predicted next day's close - today's close)
direction_svr_rbf = np.sign(pred_svr_rbf - X_test['Close'].values)
direction_svr_linear = np.sign(pred_svr_linear - X_test['Close'].values)
direction_knn = np.sign(pred_knn - X_test['Close'].values)
direction_rf = np.sign(pred_rf - X_test['Close'].values)
direction_xgb = np.sign(pred_xgb - X_test['Close'].values)

# Calculate directional accuracy (percentage of cases where the predicted direction matches the actual direction)
dir_acc_svr_rbf = np.mean(direction_svr_rbf == actual_direction) * 100
dir_acc_svr_linear = np.mean(direction_svr_linear == actual_direction) * 100
dir_acc_knn = np.mean(direction_knn == actual_direction) * 100
dir_acc_rf = np.mean(direction_rf == actual_direction) * 100
dir_acc_xgb = np.mean(direction_xgb == actual_direction) * 100

print("SVR (RBF) Directional Accuracy: {:.2f}%".format(dir_acc_svr_rbf))
print("SVR (Linear SVR) Directional Accuracy: {:.2f}%".format(dir_acc_svr_linear))
print("KNN Directional Accuracy: {:.2f}%".format(dir_acc_knn))
print("Random Forest Directional Accuracy: {:.2f}%".format(dir_acc_rf))
print("XGBoost Directional Accuracy: {:.2f}%".format(dir_acc_xgb))

"""**SAP Stock Prediction Output**"""

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.svm import SVR                 # Support Vector Regression (used as SVM for regression)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------
# Download historical data for SAP (German stock) from 2010 to 2020
data = yf.download('SAP.DE', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets (preserving time order)
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------
# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVR with linear kernel
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions on the Test Set
# ---------------------------
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# ---------------------------
# 4. Define Directional Accuracy Metric
# ---------------------------
def directional_accuracy(y_true, y_pred, baseline):
    """
    Computes directional accuracy by comparing the sign of the move (up/down)
    from the baseline (today's Close) to the actual/predicted next day's price.
    """
    # 1 indicates an upward move, 0 indicates a downward move
    actual_direction = np.where(y_true > baseline, 1, 0)
    predicted_direction = np.where(y_pred > baseline, 1, 0)
    return np.mean(actual_direction == predicted_direction) * 100

# ---------------------------
# 5. Evaluate Models using MAE, MAPE, and Directional Accuracy
# ---------------------------
# For directional accuracy, use today's Close (from X_test['Close']) as the baseline.
# Compute metrics for each model:

# SVR (RBF)
mae_svr_rbf = mean_absolute_error(y_test, pred_svr_rbf)
mape_svr_rbf = mean_absolute_percentage_error(y_test, pred_svr_rbf) * 100
dir_acc_svr_rbf = directional_accuracy(y_test.values, pred_svr_rbf, X_test['Close'].values)

# SVR (Linear)
mae_svr_linear = mean_absolute_error(y_test, pred_svr_linear)
mape_svr_linear = mean_absolute_percentage_error(y_test, pred_svr_linear) * 100
dir_acc_svr_linear = directional_accuracy(y_test.values, pred_svr_linear, X_test['Close'].values)

# KNN
mae_knn = mean_absolute_error(y_test, pred_knn)
mape_knn = mean_absolute_percentage_error(y_test, pred_knn) * 100
dir_acc_knn = directional_accuracy(y_test.values, pred_knn, X_test['Close'].values)

# Random Forest
mae_rf = mean_absolute_error(y_test, pred_rf)
mape_rf = mean_absolute_percentage_error(y_test, pred_rf) * 100
dir_acc_rf = directional_accuracy(y_test.values, pred_rf, X_test['Close'].values)

# XGBoost
mae_xgb = mean_absolute_error(y_test, pred_xgb)
mape_xgb = mean_absolute_percentage_error(y_test, pred_xgb) * 100
dir_acc_xgb = directional_accuracy(y_test.values, pred_xgb, X_test['Close'].values)

# ---------------------------
# 6. Print the Evaluation Metrics
# ---------------------------
print("SVR (RBF) -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_svr_rbf, mape_svr_rbf, dir_acc_svr_rbf))
print("SVR (Linear) -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_svr_linear, mape_svr_linear, dir_acc_svr_linear))
print("KNN -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_knn, mape_knn, dir_acc_knn))
print("Random Forest -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_rf, mape_rf, dir_acc_rf))
print("XGBoost -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_xgb, mape_xgb, dir_acc_xgb))

"""**SAMSUNG Stock Prediction Output**"""

import yfinance as yf
import pandas as pd
import numpy as np

# Import models and utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.svm import SVR                 # Support Vector Regression (SVM for regression)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

### SAMSUNG (South Korea) STOCK ANALYSIS MODEL ###

# ---------------------------
# 1. Download and Prepare Data
# ---------------------------
# Download historical data for Samsung Electronics from 2010 to 2020
data = yf.download('005930.KS', start='2010-01-01', end='2020-12-31')

# Create a target variable: next day's closing price
data['Target'] = data['Close'].shift(-1)

# Create a feature: 5-day moving average of the closing price
data['MA5'] = data['Close'].rolling(window=5).mean()

# Drop rows with missing values (from rolling calculation or shifting)
data = data.dropna()

# Define features (using today's Close and the 5-day MA) and target (next day's Close)
X = data[['Close', 'MA5']]
y = data['Target']

# Split data into training (80%) and testing (20%) sets while preserving time order
split_index = int(len(data) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Save original test features for directional accuracy (baseline is today's Close)
X_test_original = X_test.copy()

# Scale features (important for SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# 2. Initialize and Train Models
# ---------------------------
# Model 1: SVR with RBF kernel
svr_rbf = SVR(kernel='rbf')
svr_rbf.fit(X_train_scaled, y_train)

# Model 2: SVR with linear kernel
svr_linear = SVR(kernel='linear')
svr_linear.fit(X_train_scaled, y_train)

# Model 3: K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Model 4: Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Model 5: XGBoost Regression
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train_scaled, y_train)

# ---------------------------
# 3. Make Predictions on the Test Set
# ---------------------------
pred_svr_rbf = svr_rbf.predict(X_test_scaled)
pred_svr_linear = svr_linear.predict(X_test_scaled)
pred_knn = knn.predict(X_test_scaled)
pred_rf = rf.predict(X_test_scaled)
pred_xgb = xgb.predict(X_test_scaled)

# ---------------------------
# 4. Define Directional Accuracy Metric
# ---------------------------
def directional_accuracy(y_true, y_pred, baseline):
    """
    Computes directional accuracy by comparing the sign of the move (up/down)
    from the baseline (today's Close) to the actual/predicted next day's price.
    """
    # Convert baseline to numpy array if not already
    baseline = np.array(baseline)
    # 1 indicates an upward move, 0 indicates a downward move
    actual_direction = np.where(y_true > baseline, 1, 0)
    predicted_direction = np.where(y_pred > baseline, 1, 0)
    return np.mean(actual_direction == predicted_direction) * 100

# ---------------------------
# 5. Evaluate Models using MAE, MAPE, and Directional Accuracy
# ---------------------------
# For directional accuracy, use today's Close (from X_test['Close']) as the baseline.
# SVR (RBF)
mae_svr_rbf = mean_absolute_error(y_test, pred_svr_rbf)
mape_svr_rbf = mean_absolute_percentage_error(y_test, pred_svr_rbf) * 100
dir_acc_svr_rbf = directional_accuracy(y_test.values, pred_svr_rbf, X_test['Close'].values)

# SVR (Linear)
mae_svr_linear = mean_absolute_error(y_test, pred_svr_linear)
mape_svr_linear = mean_absolute_percentage_error(y_test, pred_svr_linear) * 100
dir_acc_svr_linear = directional_accuracy(y_test.values, pred_svr_linear, X_test['Close'].values)

# KNN
mae_knn = mean_absolute_error(y_test, pred_knn)
mape_knn = mean_absolute_percentage_error(y_test, pred_knn) * 100
dir_acc_knn = directional_accuracy(y_test.values, pred_knn, X_test['Close'].values)

# Random Forest
mae_rf = mean_absolute_error(y_test, pred_rf)
mape_rf = mean_absolute_percentage_error(y_test, pred_rf) * 100
dir_acc_rf = directional_accuracy(y_test.values, pred_rf, X_test['Close'].values)

# XGBoost
mae_xgb = mean_absolute_error(y_test, pred_xgb)
mape_xgb = mean_absolute_percentage_error(y_test, pred_xgb) * 100
dir_acc_xgb = directional_accuracy(y_test.values, pred_xgb, X_test['Close'].values)

# ---------------------------
# 6. Print the Evaluation Metrics
# ---------------------------
print("SVR (RBF) -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_svr_rbf, mape_svr_rbf, dir_acc_svr_rbf))
print("SVR (Linear) -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_svr_linear, mape_svr_linear, dir_acc_svr_linear))
print("KNN -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_knn, mape_knn, dir_acc_knn))
print("Random Forest -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_rf, mape_rf, dir_acc_rf))
print("XGBoost -> MAE: {:.4f}, MAPE: {:.2f}%, Directional Accuracy: {:.2f}%".format(mae_xgb, mape_xgb, dir_acc_xgb))

"""**Confussion Matrix Stock Prediction Model XGBoost**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Convert predictions into directional classes
def get_direction(actual, predicted, baseline):
    """
    Computes directional accuracy by comparing the sign of the move (up/down)
    from the baseline (today's Close) to the actual/predicted next day's price.
    """
    # 'Up' (1) if next day's price is higher than today's close, else 'Down' (0).
    actual_direction = np.where(actual > baseline, 1, 0)
    predicted_direction = np.where(predicted > baseline, 1, 0)
    return actual_direction, predicted_direction

# Get directional movement for one of the models (e.g., XGBoost)
y_actual_dir, y_pred_dir_xgb = get_direction(y_test.values, pred_xgb, X_test['Close'].values)

# Flatten the arrays to ensure correct input format for confusion_matrix
y_actual_dir_flat = y_actual_dir.ravel()
y_pred_dir_xgb_flat = y_pred_dir_xgb.ravel()

# Compute confusion matrix
cm = confusion_matrix(y_actual_dir_flat, y_pred_dir_xgb_flat)

# Plot confusion matrix
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Stock Price Direction (XGBoost)')
plt.show()

"""**Comparison of Machine Learning Models for Stock Prediction Model**"""

import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Function to train models and evaluate performance
def evaluate_stock(ticker, stock_name):
    print(f"Downloading data for {stock_name} ({ticker})...")

    # Download historical data from 2010 to 2020
    data = yf.download(ticker, start='2010-01-01', end='2020-12-31')

    # Create a target variable: next day's closing price
    data['Target'] = data['Close'].shift(-1)

    # Create a feature: 5-day moving average of the closing price
    data['MA5'] = data['Close'].rolling(window=5).mean()

    # Drop rows with missing values (from rolling calculation or shifting)
    data = data.dropna()

    # Define features and target
    X = data[['Close', 'MA5']]
    y = data['Target']

    # Split data into training (80%) and testing (20%) sets
    split_index = int(len(data) * 0.8)
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    # Save original test features for directional accuracy
    X_test_original = X_test.copy()

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Initialize and train models
    models = {
        "SVR (RBF)": SVR(kernel='rbf'),
        "SVR (Linear)": SVR(kernel='linear'),
        "KNN": KNeighborsRegressor(n_neighbors=5),
        "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
        "XGBoost": XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
    }

    predictions = {}
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        predictions[name] = model.predict(X_test_scaled)

    # Directional accuracy function
    def directional_accuracy(y_true, y_pred, baseline):
        baseline = np.array(baseline)
        actual_direction = np.where(y_true > baseline, 1, 0)
        predicted_direction = np.where(y_pred > baseline, 1, 0)
        return np.mean(actual_direction == predicted_direction) * 100

    # Evaluate models
    results = []
    for name, pred in predictions.items():
        mae = mean_absolute_error(y_test, pred)
        mape = mean_absolute_percentage_error(y_test, pred) * 100
        dir_acc = directional_accuracy(y_test.values, pred, X_test['Close'].values)
        results.append([stock_name, name, mae, mape, dir_acc])

    return results

# Run evaluations for Apple, Samsung, and SAP
stocks = [("AAPL", "Apple"), ("005930.KS", "Samsung"), ("SAP.DE", "SAP")]
all_results = []

for ticker, stock_name in stocks:
    all_results.extend(evaluate_stock(ticker, stock_name))

# Convert results to DataFrame
comparison_df = pd.DataFrame(all_results, columns=["Stock", "Model", "MAE", "MAPE (%)", "Directional Accuracy (%)"])

# Pivot table for better comparison
comparison_pivot = comparison_df.pivot(index="Model", columns="Stock", values=["MAE", "MAPE (%)", "Directional Accuracy (%)"])

# Display results
print("\nComparison of Machine Learning Models for Stock Prediction:")
print(comparison_pivot)

# Save results to CSV
comparison_df.to_csv("Stock_Comparison_Results.csv", index=False)
print("\nResults saved as 'Stock_Comparison_Results.csv'")

import joblib
joblib.dump(model_xgb, "xgb_model.pkl")
model_lstm.save("lstm_model.h5")

import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense
import matplotlib.pyplot as plt

def train_cnn_lstm(stock_symbol='AAPL', start='2015-01-01', end='2020-12-31'):
    df = yf.download(stock_symbol, start=start, end=end)
    data = df[['Close']].values

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data)

    sequence_length = 60
    X, y = [], []
    for i in range(sequence_length, len(scaled_data)):
        X.append(scaled_data[i-sequence_length:i, 0])
        y.append(scaled_data[i, 0])

    X = np.array(X)
    y = np.array(y)

    # Reshape: (samples, time steps, features)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # CNN-LSTM model
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(LSTM(100))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')

    model.fit(X, y, epochs=5, batch_size=32, verbose=1)

    # Predict
    predicted = model.predict(X)
    predicted_prices = scaler.inverse_transform(predicted)
    real_prices = scaler.inverse_transform(y.reshape(-1, 1))

    # Plot
    plt.figure(figsize=(10,6))
    plt.plot(real_prices, color='blue', label='Actual Prices')
    plt.plot(predicted_prices, color='orange', label='Predicted Prices (CNN+LSTM)')
    plt.title(f'{stock_symbol} Stock Forecast with CNN+LSTM')
    plt.xlabel('Time')
    plt.ylabel('Price')
    plt.legend()
    plt.tight_layout()
    plt.savefig("cnn_lstm_output.png")
    plt.show() # Add this line to display the plot

    return "cnn_lstm_output.png"

"""**Hybrid Model LSTM + CNN Method**"""

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense

# Step 1: Download stock data
df = yf.download("AAPL", start="2010-01-01", end="2020-12-31")[['Close']].dropna()

# Step 2: Normalize the data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

# Step 3: Create time series sequences
def create_sequences(data, seq_len=60):
    X, y = [], []
    for i in range(seq_len, len(data)):
        X.append(data[i-seq_len:i])
        y.append(data[i])
    return np.array(X), np.array(y)

seq_len = 60
X, y = create_sequences(scaled_data, seq_len)

# Step 4: Train-Test Split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Step 5: Reshape for CNN input
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Step 6: Build CNN + LSTM model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_len, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Step 7: Train model
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

# Step 8: Make predictions
predicted_scaled = model.predict(X_test)
predicted = scaler.inverse_transform(predicted_scaled)
actual = scaler.inverse_transform(y_test.reshape(-1, 1))

# Step 9: Evaluate
mse = mean_squared_error(actual, predicted)
mae = mean_absolute_error(actual, predicted)

# Optional: MAPE and Directional Accuracy
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 # Corrected syntax and added * 100

def directional_accuracy(y_true, y_pred, baseline):
    baseline = np.array(baseline)
    actual_direction = np.where(y_true > baseline, 1, 0)
    predicted_direction = np.where(y_pred > baseline, 1, 0)
    return np.mean(actual_direction == predicted_direction) * 100

mape = mean_absolute_percentage_error(actual, predicted)
dir_acc = directional_accuracy(actual.flatten(), predicted.flatten(), X_test[:, -1, 0].flatten()) # Use the last element of the sequence as baseline

print(f"CNN+LSTM Model Evaluation:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"Directional Accuracy: {dir_acc:.2f}%")

# Plot
plt.figure(figsize=(12,6))
plt.plot(actual, color='blue', label='Actual Prices')
plt.plot(predicted, color='orange', label='Predicted Prices (CNN+LSTM)')
plt.title(f'AAPL Stock Forecast with CNN+LSTM')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.tight_layout()
plt.show()

# app.py
import streamlit as st
import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense

# Set Streamlit page config
st.set_page_config(page_title="Stock Forecasting with CNN+LSTM", layout="wide")

# Title
st.title("ðŸ“Š Stock Price Forecasting with CNN + LSTM")

# Sidebar - stock selection
stock_choice = st.sidebar.selectbox(
    "Choose a Stock",
    options=["AAPL", "SAP.DE", "005930.KS"],
    format_func=lambda x: {"AAPL": "Apple", "SAP.DE": "SAP SE (Germany)", "005930.KS": "Samsung Electronics"}[x]
)

# Sidebar - epochs
epochs = st.sidebar.slider("Number of Training Epochs", min_value=5, max_value=50, value=10, step=5)

# Sidebar - sequence length
seq_len = st.sidebar.slider("Sequence Length (Days)", min_value=30, max_value=100, value=60, step=10)

# Download data
st.write(f"Downloading **{stock_choice}** stock data from 2010 to 2020...")
df = yf.download(stock_choice, start='2010-01-01', end='2020-12-31')[['Close']].dropna()

# Plot raw data
st.subheader("Historical Closing Prices")
st.line_chart(df)

# Normalize data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

# Create sequences
def create_sequences(data, seq_len=60):
    X, y = [], []
    for i in range(seq_len, len(data)):
        X.append(data[i-seq_len:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data, seq_len)
X = X.reshape((X.shape[0], X.shape[1], 1))  # reshape for CNN+LSTM

# Train-test split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Build CNN + LSTM model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_len, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
st.write(f"Training CNN+LSTM model with **{epochs} epochs**...")
history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)

# Predict on test set
pred_scaled = model.predict(X_test)
predicted = scaler.inverse_transform(pred_scaled)
actual = scaler.inverse_transform(y_test.reshape(-1,1))

# Evaluation metrics
mse = mean_squared_error(actual, predicted)
mae = mean_absolute_error(actual, predicted)
mape = np.mean(np.abs((actual - predicted) / actual)) * 100

# Directional accuracy
baseline = X_test[:, -1, 0].reshape(-1,1)
baseline_inv = scaler.inverse_transform(baseline)
actual_dir = np.where(actual > baseline_inv, 1, 0)
pred_dir = np.where(predicted > baseline_inv, 1, 0)
dir_acc = np.mean(actual_dir == pred_dir) * 100

# Show metrics
st.subheader("ðŸ“ˆ Model Performance")
st.metric("MSE", f"{mse:.4f}")
st.metric("MAE", f"{mae:.4f}")
st.metric("MAPE (%)", f"{mape:.2f}%")
st.metric("Directional Accuracy (%)", f"{dir_acc:.2f}%")

# Plot results
st.subheader("Prediction vs Actual")
fig, ax = plt.subplots(figsize=(12,5))
ax.plot(actual, label='Actual', color='blue')
ax.plot(predicted, label='Predicted', color='orange')
ax.set_title(f"{stock_choice} Price Forecast (CNN+LSTM)")
ax.set_xlabel("Time")
ax.set_ylabel("Price")
ax.legend()
st.pyplot(fig)

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# app.py
import streamlit as st
import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense

# Set Streamlit page config
st.set_page_config(page_title="Stock Forecasting with CNN+LSTM", layout="wide")

# Title
st.title("ðŸ“Š Stock Price Forecasting with CNN + LSTM")

# Sidebar - stock selection
stock_choice = st.sidebar.selectbox(
    "Choose a Stock",
    options=["AAPL", "SAP.DE", "005930.KS"],
    format_func=lambda x: {"AAPL": "Apple", "SAP.DE": "SAP SE (Germany)", "005930.KS": "Samsung Electronics"}[x]
)

# Sidebar - epochs
epochs = st.sidebar.slider("Number of Training Epochs", min_value=5, max_value=50, value=10, step=5)

# Sidebar - sequence length
seq_len = st.sidebar.slider("Sequence Length (Days)", min_value=30, max_value=100, value=60, step=10)

# Download data
st.write(f"Downloading **{stock_choice}** stock data from 2010 to 2020...")
df = yf.download(stock_choice, start='2010-01-01', end='2020-12-31')

# Flatten MultiIndex columns if they exist
if isinstance(df.columns, pd.MultiIndex):
    df.columns = ['_'.join(col).strip() for col in df.columns.values]

# Get the correct 'Close' column name based on the stock ticker
close_column_name = f'Close_{stock_choice}'

# Use only the 'Close' price column and drop NaNs
df = df[[close_column_name]].dropna()


# Plot raw data
st.subheader("Historical Closing Prices")
st.line_chart(df)

# Normalize data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

# Create sequences
def create_sequences(data, seq_len=60):
    X, y = [], []
    for i in range(seq_len, len(data)):
        X.append(data[i-seq_len:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data, seq_len)
X = X.reshape((X.shape[0], X.shape[1], 1))  # reshape for CNN+LSTM

# Train-test split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Build CNN + LSTM model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_len, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
st.write(f"Training CNN+LSTM model with **{epochs} epochs**...")
history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)

# Predict on test set
pred_scaled = model.predict(X_test)
predicted = scaler.inverse_transform(pred_scaled)
actual = scaler.inverse_transform(y_test.reshape(-1,1))

# Evaluation metrics
mse = mean_squared_error(actual, predicted)
mae = mean_absolute_error(actual, predicted)
mape = np.mean(np.abs((actual - predicted) / actual)) * 100

# Directional accuracy
baseline = X_test[:, -1, 0].reshape(-1,1)
baseline_inv = scaler.inverse_transform(baseline)
actual_dir = np.where(actual > baseline_inv, 1, 0)
pred_dir = np.where(predicted > baseline_inv, 1, 0)
dir_acc = np.mean(actual_dir == pred_dir) * 100

# Show metrics
st.subheader("ðŸ“ˆ Model Performance")
st.metric("MSE", f"{mse:.4f}")
st.metric("MAE", f"{mae:.4f}")
st.metric("MAPE (%)", f"{mape:.2f}%")
st.metric("Directional Accuracy (%)", f"{dir_acc:.2f}%")

# Plot results
st.subheader("Prediction vs Actual")
fig, ax = plt.subplots(figsize=(12,5))
ax.plot(actual, label='Actual', color='blue')
ax.plot(predicted, label='Predicted', color='orange')
ax.set_title(f"{stock_choice} Price Forecast (CNN+LSTM)")
ax.set_xlabel("Time")
ax.set_ylabel("Price")
ax.legend()
st.pyplot(fig)